<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Jack L. Good – Portfolio</title>

  <!-- Inter system font -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="style.css" />
</head>
<body>

  <!-- Navigation Bar -->
  <header>
    <nav>
      <ul>
        <li><a href="#about">About</a></li>
        <li><a href="#projects">Projects</a></li>
        <li><a href="#experiences">Experiences</a></li>
        <li><a href="#contact">Contact</a></li>
      </ul>
    </nav>
  </header>

  <!-- About Me Section -->
  <section id="about" class="section about-header">
    <img src="headshot.jpg" alt="Jack L. Good Headshot" class="headshot">
    <div class="about-details">
      <h1>Jack L. Good</h1>
      <h2>Computer Science Student @ Cornell University. In my free time I enjoy playing tennis, fly fishing, watching the Broncos play, and listening to country/Sierreño music. Always interested in contributing to projects that improve accesibilty.</h2>
      <p>
        <a href="mailto:jacklianggood@gmail.com">jacklianggood@gmail.com</a> | (720) 668-7343 | 
        <a href="https://www.linkedin.com/in/jacklgood/" target="_blank">LinkedIn</a> | 
        <a href="https://github.com/JackLGood" target="_blank">GitHub</a>
      </p>
    </div>
  </section>

  <!-- Current Projects Section -->
  <section id="projects" class="section">
    <h2>Projects</h2>
    <div class="projects-grid">
      <a href="#" class="project-card">
        <div class="card-content">
          <h3>Revenue-Optimizing Auto-Loan Pricing Engine with Random-Forest Uplift Modeling</h3>
          <ul>
            <li><strong>Built an end-to-end pricing-optimization pipeline for an online auto-lender:</strong> cleaned 40 k+ loan records, explored acceptance-vs-APR trade-offs, and used a logistic baseline to quantify current revenue; then iteratively optimized each customer’s rate, demonstrating a portfolio-wide uplift of ≈ $84 k in expected revenue for the test cohort.</li>
            <li><strong>Bench-marked multiple ML models and surfaced actionable insights:</strong> engineered features (APR, FICO, competition rate, cost of funds, partner channel), tuned SVM, Random-Forest, K-NN, and a small feed-forward ANN via cross-validated grids, selected Random-Forest (AUC ≈ 0.89; test accuracy ≈ 0.88), and visualized feature importance to show rate & loan amount drive >70 % of acceptance variance.</li>
            <li><strong>Stack:</strong> Python, pandas, scikit-learn, TensorFlow/Keras, seaborn, matplotlib.</li>
          </ul>
        </div>
      </a>
      <a href="https://github.com/JackLGood/RealTimeFacialEmotionForUnity" class="project-card">
        <div class="card-content">
          <h3>Real-Time Emotion Recognition in Unity Using Deep Convolutional Neural Networks</h3>
          <ul>
            <li><strong>Real-time 8-class emotion inference:</strong> via webcam frames fed into a FER+ DCNN using Unity’s Barracuda engine, classifying neutral, happiness, surprise, sadness, anger, disgust, fear, and contempt at interactive frame rates.</li>
            <li><strong>Precision face alignment:</strong> with OpenCV’s 68-point landmarks (eyes, nose, mouth, chin), rotating/scaling each frame to match FER+’s 64×64 grayscale input.</li>
            <li><strong>Developer-friendly controls & debug UI:</strong> including Inspector sliders for zoom, padding, smoothing, and a live preview of the exact crop sent to the network.</li>
            <li><strong>Tags:</strong> computer vision, deep convolution neural networks, Unity, OpenCV (C#)</li>
          </ul>
        </div>
      </a>
      <a href="#" class="project-card">
        <div class="card-content">
          <h3>NLP-Driven Product Clustering Uncovers Customer Spend Patterns</h3>
          <ul>
            <li><strong>Turned 500 K+ retail descriptions into a 200-term feature matrix:</strong> tokenized text, filtered nouns with NLTK POS-tagging, applied stemming, and one-hot-encoded price buckets to create a sparse SKU-feature table for downstream clustering.</li>
            <li><strong>Revealed five high-value product segments with K-Means + word-clouds:</strong> holiday gifts, fashion accessories, home-decor, lifestyle, and cozy “gift-set” items—linking each cluster to distinct spending behavior for targeted promos and inventory planning.</li>
            <li><strong>Stack:</strong> Python, pandas, NLTK, scikit-learn, seaborn, matplotlib, WordCloud.</li>
          </ul>
        </div>
      </a>
      <a href="https://github.com/JackLGood/SpectaclesWeatherApp" class="project-card">
        <div class="card-content">
          <h3>Interactive 3D Weather Visualizer with ChatGPT for AR Wearables</h3>
          <ul>
            <li><strong>AI-driven Weather Summaries on-Device:</strong> Seamlessly integrate ChatGPT with real-time OpenWeatherMap data and reverse-geocoded location info to generate on-tap, natural-language weather insights in your Spectacles AR lens.</li>
            <li><strong>Dynamic, Data-Responsive AR Pipeline:</strong> Leverage Texture2D mapping and dynamic VFX—driven by live temperature, conditions, and hourly forecasts—to render context-aware 3D models and icons that adapt instantly to changing weather.</li>
            <li><strong>Robust Asynchronous & Interaction Framework:</strong> Architect an end-to-end, gesture-driven Lens Studio experience with efficient API caching, error handling, and responsive UI components for smooth, high-performance AR on wearable hardware.</li>
            <li><strong>Stack:</strong> Augmented Reality, OpenAI ChatGPT API, Snap Lens Studio (JavaScript), Spectacles SDK, OpenWeatherMap & OpenCage APIs, WebGL/Texture2D rendering, geolocation & reverse-geocoding services.</li>
          </ul>
        </div>
      </a>

      <a href="#" class="project-card">
        <div class="card-content">
          <h3>Regularized Regression Pipeline for Airbnb Price Prediction</h3>
          <ul>
            <li><strong>Bench-marked linear, Ridge, and LASSO models on 540 Florence listings:</strong> engineered 24 host, location, and review features, visualized bias-variance trade-offs, and used 5-fold CV to pick λ values—cutting validation MSE from 4 953 (1-feature baseline) to 3 729 with Ridge.</li>
            <li><strong>Delivered interpretable pricing insights for hosts:</strong> Ridge shrank noisy coefficients, spotlighting bedrooms (+$58) and neighborhood premium (+$1.6 k in Centro Storico) while generalizing best on the unseen test set (MSE ≈ 5 124).</li>
            <li><strong>Stack:</strong> Python, pandas, scikit-learn, statsmodels, seaborn, matplotlib.</li>
          </ul>
        </div>
      </a>

      <a href="#" class="project-card">
        <div class="card-content">
          <h3>Term-Deposit Propensity Model — Ensemble Learning on Portuguese Bank Data</h3>
          <ul>
            <li><strong>Business objective:</strong> predict which retail-bank clients (<em>n ≈ 20 k</em>) will subscribe to a term deposit, enabling targeted call-center campaigns and saving ≈ 89 % wasted dials.</li>
            <li><strong>Feature engineering / ETL:</strong> 16 raw columns → numeric pipeline (mean imputation) + categorical pipeline (most-frequent imputation &amp; one-hot, 50+ dummies) in a <code>ColumnTransformer</code>; 100 % reproducible with <code>random_state = 0</code>.</li>
            <li><strong>Model selection:</strong>
              <ul>
                <li>Baselines: depth-tuned CART (AUC ≈ 0.925)</li>
                <li><strong>Random Forest (400 trees)</strong>: CV-AUC 0.941, 35 k features/sec scoring speed</li>
                <li><strong>AdaBoost (10× depth-5 stumps)</strong>: CV-AUC 0.936</li>
                <li><strong>XGBoost (50 trees, binary-logistic)</strong>: **best** CV-AUC 0.945 • validation AUC 0.946 • test AUC 0.930</li>
              </ul>
            </li>
            <li><strong>Class-imbalance strategy:</strong> used ROC-AUC as primary metric (only 11 % positives) plus confusion-matrix to monitor false-negatives vs outbound-call cost.</li>
            <li><strong>Impact:</strong> XGBoost model lifts AUC by +2 pp over Random Forest baseline and cuts false-positive rate by 10 %, delivering a cleaner, more profitable lead list to marketing ops.</li>
            <li><strong>Stack:</strong> Python, scikit-learn, XGBoost, pandas, NumPy, seaborn.</li>
          </ul>
        </div>
      </a>

      <a href="#" class="project-card">
        <div class="card-content">
          <h3>LSTM-Driven Forecasting of Coca-Cola (KO) Stock Prices</h3>
          <ul>
            <li><strong>Built a full time-series pipeline from Yahoo Finance ingest to production-ready forecasts:</strong> pulled 10 yrs of KO closes, engineered rolling-mean baselines, framed a 90-day sliding window, and trained a 2-layer LSTM with early-stopping—cutting MAPE from 1.25 % (10-day MA) to < 0.8 % on the 20 % hold-out set.</li>
            <li><strong>Demonstrated sequence-learning gains over classical methods:</strong> tuned 64/32-unit LSTM stack, achieved RMSE ≈ 0.42 and tracked regime shifts in post-COVID recovery, illustrating how deep RNNs capture long-horizon patterns that moving averages miss.</li>
            <li><strong>Stack:</strong> Python, pandas, yfinance, TensorFlow/Keras, NumPy, scikit-learn, seaborn, matplotlib.</li>
          </ul>
        </div>
      </a>

      <a href="#" class="project-card">
        <div class="card-content">
          <h3>High-Accuracy Crop Recommendation Engine with K-NN & Soil-Climate Profiling</h3>
          <ul>
            <li><strong>Built a precision-ag pipeline from raw agronomic data to deployable recommender:</strong> merged rainfall, nutrient, and climate tables into 2 k+ farm records, visualized N-P-K vs. humidity/temperature, then tuned a k-NN classifier (k = 3) via 5-fold CV to hit 97 % test accuracy on 22 crop classes.</li>
            <li><strong>Drove explainable, per-farm decisions for growers:</strong> standardized features to optimize Euclidean distance, generated similarity diagnostics (e.g., maize vs. peas soil demands), and delivered instant “what-if” predictions—recommending rice for sample N20 P40 K55/26 °C/7.2 pH scenarios.</li>
            <li><strong>Stack:</strong> Python, pandas, scikit-learn (StandardScaler, KNeighborsClassifier, GridSearchCV), seaborn, matplotlib.</li>
          </ul>
        </div>
      </a>
      
      <a href="#" class="project-card">
        <div class="card-content">
          <h3>Wine-Quality Classification with Linear & RBF Support-Vector Machines</h3>
          <ul>
            <li><strong>Modeled Portuguese “Vinho Verde” quality from chemistry:</strong> explored 1.6 k red-wine samples, visualized alcohol vs. volatile-acidity correlations, and framed a binary “good vs. bad” task; tuned C on linear SVM (grid 0.001 → 0.02) and RBF SVM (C = 0.5 → 20) via 5-fold CV, lifting test accuracy from 72 % (2-feature model) to 77 % (full RBF).</li>
            <li><strong>Drove margin-vs-fit analysis & baseline benchmarking:</strong> plotted hyperplanes for extreme C values to illustrate over/under-fitting, contrasted SVM with logistic regression (75 % test accuracy), and surfaced key drivers—high alcohol and low volatile acidity—as top quality indicators.</li>
            <li><strong>Stack:</strong> Python, pandas, scikit-learn (SVC, GridSearchCV, StandardScaler), seaborn, matplotlib.</li>
          </ul>
        </div>
      </a>

      <a href="#" class="project-card">
        <div class="card-content">
          <h3>Voter-Turnout Modeling & Cut-Off Tuning with Logistic Regression</h3>
          <ul>
            <li><strong>Predicted 2020 Georgia ballot-casting using 246 k voter records:</strong> engineered demographic and census-tract features (age, race, gender, income, education, car ownership, density), fit a non-regularized logistic model, and interpreted coefficients to surface key drivers (older, higher-income, female voters most likely to turn out).</li>
            <li><strong>Optimized decision thresholds & evaluated fairness trade-offs:</strong> generated ROC/accuracy-vs-cutoff curves, confusion-matrix analytics, and scenario tests (voter A/B/C) to illustrate how varying income or age shifts predicted probabilities; recommended 0.5 cutoff for balanced 71 % test accuracy with minimal over-fit.</li>
            <li><strong>Stack:</strong> Python, pandas, scikit-learn (LogisticRegression, confusion_matrix, RocCurveDisplay), seaborn, matplotlib.</li>
          </ul>
        </div>
      </a>

      <a href="#" class="project-card">
        <div class="card-content">
          <h3>Fashion-MNIST Image Classifier — 300-Neuron ANN in TensorFlow</h3>
          <ul>
            <li><strong>Engineered and trained a lean feed-forward network on 60 k gray-scale fashion images:</strong> flattened 28×28 inputs, used a single 300-ReLU hidden layer, tuned epochs via learning-curve inspection (≃ 12) to hit 85 % train / 82 % test accuracy — matching Random Forest performance while cutting inference latency to milliseconds.</li>
            <li><strong>Bench-marked against five classic ML baselines (LogReg, DT, RF, KNN, XGBoost):</strong> measured wall-time vs. accuracy/AUC, showing the ANN delivers comparable accuracy to XGBoost (0.825 vs 0.894) but trains ≈ 3× faster than heavy tree ensembles on CPU; visualized mis-classified samples to guide future CNN upgrades.</li>
            <li><strong>Stack:</strong> Python, TensorFlow/Keras, NumPy, pandas, scikit-learn, matplotlib.</li>
          </ul>
        </div>
      </a>

    </div>
    <button id="show-more-btn" class="show-more-btn">
      Show more projects
    </button>
  </section>

  <!-- Experiences Section -->
  <section id="experiences" class="section">
    <h2>Experiences</h2>
    <div class="section-content">
      <h3>Education</h3>
      <ul>
        <li><strong>Cornell University</strong> – College of Arts & Sciences/Bowers College of Computing and Information Science</li>
        <li><strong>Relevant Coursework:</strong> Machine Learning, Computer Vision, AI Reasoning & Decision, Database Systems, Data Structures & Object-Oriented Programming, Computer System Organization, Functional Programming, Discrete Structures, and other good stuff</li>
      </ul>

      <h3>Technical Skills</h3>
      <ul>
        <li><strong>Languages:</strong> Python, Java, SQL, OCaml, C#, C++, JavaScript</li>
        <li><strong>Frameworks/Libs:</strong> Pytorch, scikit-learn, NumPy, pandas, OpenCV, DeepFace, LangChain, OpenAI, REST APIs</li>
        <li><strong>Tools:</strong> Git, Tableau, Stata, Figma, VS Code, Unity (C#/AR), LaTeX</li>
      </ul>

      <h3>Professional Experience</h3>
      <h4>Cornell Tech – Undergraduate Researcher (Jun 2025 – Aug 2025)</h4>
      <ul>
        <li>ML/VLM applications in emotion detection and regulation</li>
      </ul>
      <h4>ATLAS Institute – Software Dev Intern (May 2024 – Aug 2024)</h4>
      <ul>
        <li>Human AI interaction research</li>
      </ul>

      <h4>Platt Park Capital – Data Intern (May 2023 – Aug 2023)</h4>
      <ul>
        <li>Healthcare and multi-site consumer</li>
      </ul>

      <h4>University of Colorado – Computational Chemistry Aide (Oct 2021 – Aug 2022)</h4>
      <ul>
        <li>Theoretical chemistry</li>
      </ul>
    </div>
  </section>

  <!-- Contact Section -->
  <section id="contact" class="section">
    <h2>Contact</h2>
    <div class="section-content">
      <p><strong>Email:</strong> <a href="mailto:jacklianggood@gmail.com">jacklianggood@gmail.com</a></p>
      <p><strong>Phone:</strong> (720) 668-7343</p>
      <p><strong>Other:</strong> Mutual Investment Club of Cornell, Phi Chi Theta Professional Business Fraternity.</p>
    </div>
  </section>

  <!-- Footer -->
  <footer>
    <p>© 2025 Jack L. Good</p>
  </footer>

  <script src="script.js"></script>
</body>
</html>
